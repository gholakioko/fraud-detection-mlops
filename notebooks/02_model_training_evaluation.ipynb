{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection - Model Training and Evaluation\n",
    "\n",
    "This notebook focuses on:\n",
    "- Data preprocessing and feature engineering\n",
    "- Training multiple fraud detection models\n",
    "- Comprehensive model evaluation and comparison\n",
    "- Model selection and hyperparameter tuning\n",
    "- Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.data_processor import FraudDataProcessor\n",
    "from models.fraud_detector import FraudDetector\n",
    "from utils.helpers import generate_sample_data, calculate_model_metrics\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or generate data\n",
    "data_path = '../data/raw/sample_fraud_data.csv'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Generating sample data...\")\n",
    "    df = generate_sample_data(n_samples=10000, fraud_rate=0.1)\n",
    "    os.makedirs('../data/raw', exist_ok=True)\n",
    "    df.to_csv(data_path, index=False)\n",
    "    print(f\"Sample data saved to {data_path}\")\n",
    "else:\n",
    "    print(\"Loading existing data...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud rate: {df['is_fraud'].mean()*100:.2f}%\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data processor\n",
    "data_processor = FraudDataProcessor()\n",
    "\n",
    "# Clean data\n",
    "df_clean = data_processor.clean_data(df)\n",
    "print(f\"Cleaned data shape: {df_clean.shape}\")\n",
    "\n",
    "# Feature engineering\n",
    "df_processed = data_processor.feature_engineering(df_clean)\n",
    "print(f\"Processed data shape: {df_processed.shape}\")\n",
    "print(f\"Features: {df_processed.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X, y = data_processor.prepare_features(df_processed, target_column='is_fraud')\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Feature names: {data_processor.feature_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Splitting and Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training fraud rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Test fraud rate: {y_test.mean()*100:.2f}%\")\n",
    "\n",
    "# Baseline accuracy (if we predicted everything as non-fraud)\n",
    "baseline_accuracy = (y_test == 0).mean()\n",
    "print(f\"\\nBaseline accuracy (predict all non-fraud): {baseline_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to train\n",
    "models_to_train = ['random_forest', 'logistic_regression', 'isolation_forest']\n",
    "trained_models = {}\n",
    "model_results = {}\n",
    "\n",
    "print(\"Training multiple fraud detection models...\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_type in models_to_train:\n",
    "    print(f\"\\nTraining {model_type.replace('_', ' ').title()}...\")\n",
    "    \n",
    "    # Initialize and train model\n",
    "    detector = FraudDetector(model_type=model_type)\n",
    "    \n",
    "    # For isolation forest, we only use training data\n",
    "    if model_type == 'isolation_forest':\n",
    "        # Train only on non-fraud data for unsupervised learning\n",
    "        X_train_normal = X_train[y_train == 0]\n",
    "        detector.model.fit(X_train_normal)\n",
    "        detector.is_trained = True\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = detector.predict(X_test)\n",
    "        \n",
    "        # For isolation forest, we don't have predict_proba in the same way\n",
    "        scores = detector.model.decision_function(X_test)\n",
    "        # Normalize scores to probabilities (approximate)\n",
    "        scores_normalized = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "        y_prob = 1 - scores_normalized  # Invert because lower scores mean more anomalous\n",
    "    else:\n",
    "        # Regular supervised training\n",
    "        detector.model.fit(X_train, y_train)\n",
    "        detector.is_trained = True\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = detector.predict(X_test)\n",
    "        y_prob = detector.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Store model\n",
    "    trained_models[model_type] = detector\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = calculate_model_metrics(y_test, y_pred, y_prob)\n",
    "    model_results[model_type] = results\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Results for {model_type.replace('_', ' ').title()}:\")\n",
    "    print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {results['precision']:.4f}\")\n",
    "    print(f\"  Recall: {results['recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {results['f1_score']:.4f}\")\n",
    "    if 'auc_score' in results:\n",
    "        print(f\"  AUC-ROC: {results['auc_score']:.4f}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(model_results).T\n",
    "comparison_df = comparison_df[['accuracy', 'precision', 'recall', 'f1_score']]\n",
    "comparison_df.columns = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['skyblue', 'lightgreen', 'salmon', 'orange']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    comparison_df[metric].plot(kind='bar', ax=ax, color=colors[i], alpha=0.7)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, v in enumerate(comparison_df[metric]):\n",
    "        ax.text(j, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for model_type, detector in trained_models.items():\n",
    "    if model_type == 'isolation_forest':\n",
    "        # For isolation forest, use decision function\n",
    "        scores = detector.model.decision_function(X_test)\n",
    "        scores_normalized = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "        y_prob = 1 - scores_normalized\n",
    "    else:\n",
    "        y_prob = detector.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc_score = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    plt.plot(fpr, tpr, linewidth=2, \n",
    "             label=f'{model_type.replace(\"_\", \" \").title()} (AUC = {auc_score:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for model_type, detector in trained_models.items():\n",
    "    if model_type == 'isolation_forest':\n",
    "        scores = detector.model.decision_function(X_test)\n",
    "        scores_normalized = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "        y_prob = 1 - scores_normalized\n",
    "    else:\n",
    "        y_prob = detector.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    avg_precision = average_precision_score(y_test, y_prob)\n",
    "    \n",
    "    plt.plot(recall, precision, linewidth=2,\n",
    "             label=f'{model_type.replace(\"_\", \" \").title()} (AP = {avg_precision:.3f})')\n",
    "\n",
    "# Baseline\n",
    "baseline_precision = y_test.mean()\n",
    "plt.axhline(y=baseline_precision, color='k', linestyle='--', linewidth=1, \n",
    "            label=f'Baseline (AP = {baseline_precision:.3f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (model_type, detector) in enumerate(trained_models.items()):\n",
    "    y_pred = detector.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=['Non-Fraud', 'Fraud'],\n",
    "                yticklabels=['Non-Fraud', 'Fraud'],\n",
    "                ax=axes[i])\n",
    "    \n",
    "    axes[i].set_title(f'{model_type.replace(\"_\", \" \").title()}\\nConfusion Matrix', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "    \n",
    "    # Add raw counts as text\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            axes[i].text(k+0.5, j+0.75, f'n={cm[j,k]}', \n",
    "                        ha='center', va='center', fontsize=10, color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "feature_importance_models = ['random_forest', 'logistic_regression']\n",
    "\n",
    "for i, model_type in enumerate(feature_importance_models):\n",
    "    detector = trained_models[model_type]\n",
    "    \n",
    "    if model_type == 'random_forest':\n",
    "        importances = detector.model.feature_importances_\n",
    "        title = 'Random Forest Feature Importance'\n",
    "    elif model_type == 'logistic_regression':\n",
    "        importances = np.abs(detector.model.coef_[0])\n",
    "        title = 'Logistic Regression Feature Importance\\n(Absolute Coefficients)'\n",
    "    \n",
    "    # Create dataframe for better handling\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': data_processor.feature_columns,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    # Plot\n",
    "    feature_importance_df.plot(x='feature', y='importance', kind='barh', \n",
    "                               ax=axes[i], color='lightcoral', alpha=0.7)\n",
    "    axes[i].set_title(title, fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Importance')\n",
    "    axes[i].set_ylabel('Features')\n",
    "    axes[i].legend().set_visible(False)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(\"=\" * 40)\n",
    "for model_type in feature_importance_models:\n",
    "    detector = trained_models[model_type]\n",
    "    \n",
    "    if model_type == 'random_forest':\n",
    "        importances = detector.model.feature_importances_\n",
    "    else:\n",
    "        importances = np.abs(detector.model.coef_[0])\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': data_processor.feature_columns,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{model_type.replace('_', ' ').title()}:\")\n",
    "    for _, row in feature_importance_df.head().iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for supervised models\n",
    "cv_results = {}\n",
    "cv_folds = 5\n",
    "cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Performing {cv_folds}-fold cross-validation...\\n\")\n",
    "\n",
    "supervised_models = ['random_forest', 'logistic_regression']\n",
    "\n",
    "for model_type in supervised_models:\n",
    "    detector = FraudDetector(model_type=model_type)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(detector.model, X_train, y_train, \n",
    "                                cv=cv, scoring='f1')\n",
    "    \n",
    "    cv_results[model_type] = cv_scores\n",
    "    \n",
    "    print(f\"{model_type.replace('_', ' ').title()}:\")\n",
    "    print(f\"  CV F1-Scores: {cv_scores}\")\n",
    "    print(f\"  Mean F1-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "cv_data = [cv_results[model] for model in supervised_models]\n",
    "model_names = [model.replace('_', ' ').title() for model in supervised_models]\n",
    "\n",
    "box_plot = plt.boxplot(cv_data, labels=model_names, patch_artist=True)\n",
    "\n",
    "colors = ['lightblue', 'lightgreen']\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "plt.title('Cross-Validation F1-Scores Distribution', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.xlabel('Model')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Selection and Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on F1-score\n",
    "best_model_name = max(model_results.keys(), \n",
    "                      key=lambda x: model_results[x]['f1_score'])\n",
    "best_model = trained_models[best_model_name]\n",
    "best_results = model_results[best_model_name]\n",
    "\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Selected Model: {best_model_name.replace('_', ' ').title()}\")\n",
    "print(f\"Selection Criteria: Highest F1-Score\")\n",
    "\n",
    "print(f\"\\nBest Model Performance:\")\n",
    "print(f\"  Accuracy: {best_results['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {best_results['precision']:.4f}\")\n",
    "print(f\"  Recall: {best_results['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {best_results['f1_score']:.4f}\")\n",
    "if 'auc_score' in best_results:\n",
    "    print(f\"  AUC-ROC: {best_results['auc_score']:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Non-Fraud', 'Fraud']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename = f\"../models/trained/best_fraud_detector_{best_model_name}_{timestamp}.pkl\"\n",
    "\n",
    "os.makedirs('../models/trained', exist_ok=True)\n",
    "best_model.save_model(model_filename)\n",
    "\n",
    "# Save results\n",
    "results_filename = f\"../models/trained/model_evaluation_results_{timestamp}.json\"\n",
    "\n",
    "import json\n",
    "with open(results_filename, 'w') as f:\n",
    "    json.dump({\n",
    "        'best_model': best_model_name,\n",
    "        'best_model_results': best_results,\n",
    "        'all_model_results': model_results,\n",
    "        'training_info': {\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test),\n",
    "            'features': data_processor.feature_columns,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nMODEL AND RESULTS SAVED\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Best model saved to: {model_filename}\")\n",
    "print(f\"Evaluation results saved to: {results_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"MODEL TRAINING AND EVALUATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n1. DATA OVERVIEW:\")\n",
    "print(f\"   • Total samples: {len(df)}\")\n",
    "print(f\"   • Training samples: {len(X_train)}\")\n",
    "print(f\"   • Test samples: {len(X_test)}\")\n",
    "print(f\"   • Features used: {len(data_processor.feature_columns)}\")\n",
    "print(f\"   • Fraud rate: {y.mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n2. MODELS EVALUATED:\")\n",
    "for model_type, results in model_results.items():\n",
    "    print(f\"   • {model_type.replace('_', ' ').title()}: F1={results['f1_score']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. BEST MODEL:\")\n",
    "print(f\"   • Model: {best_model_name.replace('_', ' ').title()}\")\n",
    "print(f\"   • F1-Score: {best_results['f1_score']:.4f}\")\n",
    "print(f\"   • Precision: {best_results['precision']:.4f}\")\n",
    "print(f\"   • Recall: {best_results['recall']:.4f}\")\n",
    "\n",
    "print(f\"\\n4. BUSINESS IMPACT:\")\n",
    "# Calculate business metrics\n",
    "total_fraud_cases = y_test.sum()\n",
    "detected_fraud_cases = confusion_matrix(y_test, y_pred_best)[1, 1]\n",
    "false_positives = confusion_matrix(y_test, y_pred_best)[0, 1]\n",
    "\n",
    "print(f\"   • Total fraud cases in test set: {total_fraud_cases}\")\n",
    "print(f\"   • Detected fraud cases: {detected_fraud_cases} ({detected_fraud_cases/total_fraud_cases*100:.1f}%)\")\n",
    "print(f\"   • False positives: {false_positives} ({false_positives/len(y_test)*100:.1f}% of all transactions)\")\n",
    "\n",
    "print(f\"\\n5. RECOMMENDATIONS:\")\n",
    "print(f\"   • Deploy {best_model_name.replace('_', ' ').title()} as production model\")\n",
    "print(f\"   • Monitor model performance with emphasis on recall (fraud detection rate)\")\n",
    "print(f\"   • Consider threshold tuning based on business cost of false positives vs false negatives\")\n",
    "print(f\"   • Implement regular model retraining with new data\")\n",
    "print(f\"   • Set up alerting for model performance degradation\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"MODEL TRAINING AND EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"Ready for production deployment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
